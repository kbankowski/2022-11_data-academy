{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"float:left;\"><img src=\"logo.png\" width=\"500\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Node Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This demo will focus on more advanced topics, specifically using a Graph Convolutional Network (GCN) for node classification. The notebook requires the *DGL* and *PyTorch* Python packages.\n",
    "\n",
    "For installation instruction for DGL, see:\n",
    "https://www.dgl.ai/pages/start.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In network analysis **node classification** is a widely-applied task which training a machine learning model to classify the nodes in a network into two or more classes or categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import itertools, random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "# imports for DGL\n",
    "import dgl\n",
    "import dgl.data\n",
    "import dgl.function as fn\n",
    "from dgl.nn import GraphConv\n",
    "\n",
    "# display settings\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams.update({'font.size': 14})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up number generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_all_seeds(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "set_all_seeds(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "Load the \"Cora\" scientific network dataset, which is included as part of DGL. \n",
    "\n",
    "Note that we could also create a NetworkX graph and convert it for use with DGL, using *dgl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dgl.data.CoraGraphDataset()\n",
    "# check the structure of the associated network\n",
    "g = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = dataset.num_classes\n",
    "print(\"Dataset has %d class labels\" % num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Cora dataset includes a predefined training/validation/test split of the nodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = g.ndata['feat']\n",
    "labels = g.ndata['label']\n",
    "# get the masks for the training/validation/test nodes\n",
    "train_mask = g.ndata['train_mask']\n",
    "val_mask = g.ndata['val_mask']\n",
    "test_mask = g.ndata['test_mask']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As our architecture we will use a two-layer Graph Convolutional Network (GCN), where each layer computes new node representations by aggregating neighbour information from the nodes. This is like the idea of node embeddings that we saw previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the GCN model\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_feats, h_feats, num_classes):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GraphConv(in_feats, h_feats)\n",
    "        self.conv2 = GraphConv(h_feats, num_classes)\n",
    "\n",
    "    def forward(self, g, in_feat):\n",
    "        h = self.conv1(g, in_feat)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv2(g, h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the model and optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the GCN model with given dimensions\n",
    "model = GCN(g.ndata['feat'].shape[1], 16, num_classes)\n",
    "# create the optimisation function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply training for the specified number of epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 100\n",
    "loss_scores, train_acc, val_acc, test_acc = {}, {}, {}, {}\n",
    "for e in range(1, max_epochs+1):\n",
    "    # Forward\n",
    "    logits = model(g, features)\n",
    "    # Compute the prediction\n",
    "    pred = logits.argmax(1)\n",
    "    \n",
    "    # Compute the loss on the training set\n",
    "    loss = F.cross_entropy(logits[train_mask], labels[train_mask])\n",
    "    loss_scores[e] = float(loss)\n",
    "\n",
    "    # Compute accuracy on each of the training/validation/test sets at each epoch\n",
    "    train_acc[e] = float((pred[train_mask] == labels[train_mask]).float().mean())\n",
    "    val_acc[e] = float((pred[val_mask] == labels[val_mask]).float().mean())\n",
    "    test_acc[e] = float((pred[test_mask] == labels[test_mask]).float().mean())\n",
    "\n",
    "    # Backward\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if e % 20 == 0:\n",
    "        print('Epoch %d/%d' % (e, max_epochs))      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the trajectory of the loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = pd.Series(loss_scores).plot(figsize=(10,6), zorder=3)\n",
    "ax.set_xlabel(\"Training Epoch\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.yaxis.grid()\n",
    "ax.set_xlim(1, max_epochs)\n",
    "ax.set_ylim(0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the accuracy scores for each of the splits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_acc = pd.DataFrame({\"Train\": pd.Series(train_acc), \n",
    "                      \"Validation\": pd.Series(val_acc), \"Test\": pd.Series(test_acc)})\n",
    "ax = df_acc.plot(figsize=(10, 6), zorder=3)\n",
    "ax.set_xlabel(\"Training Epoch\")\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "ax.yaxis.grid()\n",
    "plt.legend(loc='lower right')\n",
    "ax.set_xlim(1, max_epochs)\n",
    "ax.set_ylim(0);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
