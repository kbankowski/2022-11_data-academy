{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECB Data Academy - Week 5 - Ensembles\n",
    "[Krisolis](http://www.krisolis.ie)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â Different Model Types in sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how diferent model types can be trained in **sklearn**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Package Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build predictive models in Python we use a set of libraries that are imported here. In particular **pandas** and **sklearn** are particularly important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Saving Python ojects\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# General data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Drawing plots\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas_profiling\n",
    "\n",
    "# Machine learning\n",
    "import sklearn\n",
    "import sklearn.impute\n",
    "import sklearn.model_selection\n",
    "import sklearn.metrics\n",
    "import sklearn.tree\n",
    "import sklearn.svm\n",
    "import sklearn.ensemble\n",
    "import sklearn.linear_model\n",
    "import sklearn.neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load & Partition Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To support data exploration and manipulation it is easiest o load datasets as **pandas DataFrames**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('../Data/ACME_ABT.csv')\n",
    "display(dataset.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the distribution of the two classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset[\"churn\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a suite of summary statistics for the numeric and categorical features in the data, and count missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Print descriptive statsitcs for each column\n",
    "print(\"Summary Stats\")\n",
    "if dataset.select_dtypes(include=[np.number]).shape[1] > 0: \n",
    "    display(dataset.describe(include=\"number\").transpose())\n",
    "if dataset.select_dtypes(include=[object]).shape[1] > 0: \n",
    "    display(dataset.describe(include=\"object\").transpose())\n",
    "\n",
    "# Check for presence of missing values\n",
    "print(\"Missing Values\")\n",
    "print(dataset.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **ProfileReport** from **pandas_profiling** gives a very useful summary of the dataset and highliughts potential issues. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pandas_profiling.ProfileReport(dataset, \n",
    "                               minimal = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A bug in pandas_profiler means plots don;t appear after calling it. This re-import of matplotlib fixes the bug.\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare Data for Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We select features, imp-iuts missing values, replace spurious values and change categorical features to numeric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = dataset[['age',\n",
    " 'income',\n",
    " 'numHandsets',  \n",
    " 'handsetAge',\n",
    " 'smartPhone',\n",
    " 'currentHandsetPrice',\n",
    " 'avgBill',\n",
    " 'avgOverBundleMins',\n",
    " 'avgRoamCalls',\n",
    " 'callMinutesChangePct',\n",
    " 'callMinutesChangePct',\n",
    " 'billAmountChangePct',\n",
    " 'billAmountChangePct',\n",
    " 'avgReceivedMins',\n",
    " 'avgOutCalls',\n",
    " 'avgInCalls',\n",
    " 'peakOffPeakRatio',\n",
    " 'peakOffPeakRatioChangePct',\n",
    " 'avgDroppedCalls',\n",
    " 'avgDroppedCalls',\n",
    " 'lifeTime',\n",
    " 'newFrequentNumbers',\n",
    " 'regionType',\n",
    " 'marriageStatus',\n",
    " 'creditRating']]\n",
    "y = dataset[\"churn\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.loc[X['regionType'] == 's','regionType'] = \"suburban\"\n",
    "X.loc[X['regionType'] == 't','regionType'] = \"town\"\n",
    "X.loc[X['regionType'] == 'r','regionType'] = \"rural\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regionType_imputer = sklearn.impute.SimpleImputer(strategy=\"most_frequent\")\n",
    "regionType_imputer.fit(X['regionType'].values.reshape(-1, 1))\n",
    "X['regionType'] = regionType_imputer.transform(X['regionType'].values.reshape(-1, 1))\n",
    "\n",
    "age_imputer = sklearn.impute.SimpleImputer(missing_values = 0, strategy=\"mean\")\n",
    "age_imputer.fit(X['age'].values.reshape(-1, 1))\n",
    "X['age'] = age_imputer.transform(dataset['age'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creditRating_oe = sklearn.preprocessing.OrdinalEncoder()\n",
    "creditRating_oe.fit(X['creditRating'].values.reshape(-1, 1))\n",
    "X['creditRating'] = creditRating_oe.transform(X['creditRating'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.get_dummies(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examine Transformed Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the transformed dataset before modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "display(X.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pandas_profiling.ProfileReport(X, \n",
    "                               minimal = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A bug in pandas_profiler means plots don;t appear after calling it. This re-import of matplotlib fixes the bug.\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Partition Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into a **training set**, a **validation set**, and a **test set**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train_plus_valid, X_test, y_train_plus_valid, y_test \\\n",
    "    = sklearn.model_selection.train_test_split(X, y, random_state=0,\n",
    "                                               train_size = 0.7, \n",
    "                                               stratify = y)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid \\\n",
    "    = sklearn.model_selection.train_test_split(X_train_plus_valid, \n",
    "                                               y_train_plus_valid, \n",
    "                                               random_state=0, \n",
    "                                               train_size = 0.5/0.7,\n",
    "                                               stratify=y_train_plus_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the partitions created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "display(X_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_valid.shape)\n",
    "display(X_valid.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train.shape)\n",
    "display(y_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Different Model Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the great things about using **sklearn** is that all of the model types use the same pattern so changing to other model types is very straight forward. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging is a basic ensemble model approach to machine learning. The **BaggingClassifier** model object in **sklearn** implements bagging. The key parmaeters when creating a **BaggingClassifier** model are:\n",
    "\n",
    "- **base_estimator** = None: The base model to use in the ensemble.\n",
    "- **n_estimators** = 10: The number of boosting stages to perform.\n",
    "- **max_samples** = 1.0: The number of samples to draw from X with replacement to train each base estimator.\n",
    "- **max_features** = 1.0: The number of features to draw from X to train each base estimator (either a percentage or a number of features). \n",
    "- **bootstrap** = True: True for sampling with repalcement, without otherwise.\n",
    "- **n_jobs** = 1: Number of jobs to run in parallel. -1 uses all available. \n",
    "- **verbose**=0: Controls how much output will be produced when methods are called - can be 0 (no output), 1, or 2 (maximum output). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the same job with random forests\n",
    "my_model = sklearn.ensemble.BaggingClassifier(base_estimator = \n",
    "                                              sklearn.tree.DecisionTreeClassifier(criterion=\"entropy\", \n",
    "                                                            min_samples_leaf = 0.05), \\\n",
    "                                              n_estimators=50)\n",
    "my_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a set of predictions for the test data\n",
    "y_pred = my_model.predict(X_valid)\n",
    "\n",
    "# Print performance details\n",
    "print(sklearn.metrics.classification_report(y_valid, y_pred))\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix\")\n",
    "pd.crosstab(y_valid, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the parameter grid to seaerch\n",
    "param_grid = [\n",
    " {'n_estimators': list(range(5, 25, 1)),\n",
    " 'base_estimator': [sklearn.tree.DecisionTreeClassifier(), sklearn.linear_model.LogisticRegression(max_iter = 1000)]}\n",
    "]\n",
    "\n",
    "# Perform the search\n",
    "my_tuned_model = sklearn.model_selection.GridSearchCV(sklearn.ensemble.BaggingClassifier(), param_grid, cv=5)\n",
    "my_tuned_model.fit(X_train_plus_valid, y_train_plus_valid)\n",
    "\n",
    "# Print details\n",
    "print(\"Best parameters set found on development set:\")\n",
    "print(my_tuned_model.best_params_)\n",
    "print(my_tuned_model.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging is a basic ensemble model approach to machine learning. The **RandomForestClassifier** model object in **sklearn** implements the bagging. The key parmaeters when creating a **RandomForestClassifier** model are:\n",
    "\n",
    "- **criterion** = \"gini\": the criterion used for sselecting partitions during training. One of either \"entropy\" or \"gini\".\n",
    "- **splitter** = \"best\": The approach used to split numeric data at each node in the tree. One of either \"random\" or \"best\".\n",
    "- **max_depth** = None: The maximum depth that the tree is allowed to grow to. \n",
    "- **min_samples_split** = 2: The minimum number of samples required to split an internal node. \n",
    "- **min_samples_leaf** = 1: The minimum number of samples required to be at a leaf node.\n",
    "- **n_estimators** = 100: The number of boosting stages to perform.\n",
    "- **max_samples** = 1.0: The number of samples to draw from X with replacement to train each base estimator.\n",
    "- **max_features** = 1.0: The number of features to draw from X to train each base estimator (either a percentage or a number of features). \n",
    "- **bootstrap** = True: True for sampling with repalcement, without otherwise.\n",
    "- **n_jobs** = 1: Number of jobs to run in parallel. -1 uses all available. \n",
    "- **verbose**=0: Controls how much output will be produced when methods are called - can be 0 (no output), 1, or 2 (maximum output). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Do the same job with random forests\n",
    "my_model = sklearn.ensemble.RandomForestClassifier(n_estimators=300, \\\n",
    "                                           max_features = 3,\\\n",
    "                                           min_samples_split=200)\n",
    "my_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Make a set of predictions for the test data\n",
    "y_pred = my_model.predict(X_valid)\n",
    "\n",
    "# Print performance details\n",
    "print(sklearn.metrics.classification_report(y_valid, y_pred))\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix\")\n",
    "pd.crosstab(y_valid, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set up the parameter grid to seaerch\n",
    "param_grid = [\n",
    " {'n_estimators': list(range(100, 501, 50)), 'max_features': list(range(1, 10, 2)), 'min_samples_split': list(range(20, 200, 50)) }\n",
    "]\n",
    "\n",
    "# Perform the search\n",
    "my_tuned_model = sklearn.model_selection.GridSearchCV(sklearn.ensemble.RandomForestClassifier(), param_grid, cv=5)\n",
    "my_tuned_model.fit(X_train_plus_valid, y_train_plus_valid)\n",
    "\n",
    "# Print details\n",
    "print(\"Best parameters set found on development set:\")\n",
    "print(my_tuned_model.best_params_)\n",
    "print(my_tuned_model.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic regression** models are a simple approach to binary classification. The **LogisticRegression** model object in **sklearn** implements logistic regression. The key parmaeters when creating a **LogisticRegression** model are:\n",
    "\n",
    "- **penalty** = 'l2': Used to specify the type of regularisation used (one of 'l1', 'l2', 'elasticnet', or 'none').\n",
    "- **C** = 1.0: Inverse of regularization strength. Must be a positive and smaller values specify stronger regularization.\n",
    "- **warm_start** = False: When set to True, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution.\n",
    "- **max_iter** = 100: aximum number of iterations taken for the solvers to converge.\n",
    "- **n_jobs** = 1: Number of jobs to run in parallel. -1 uses all available. \n",
    "- **verbose**=0: Controls how much output will be produced when methods are called - can be 0 (no output), 1, or 2 (maximum output). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression models really struggle to fit unless data is scaled to smalle ranges (e.g. -1 to 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = X_train.columns     # Save column names to avoid lsoing them when changing from pandas dataframe to numpy array\n",
    "min_max_scaler = sklearn.preprocessing.MinMaxScaler(feature_range=(-1,1))\n",
    "min_max_scaler.fit(X_train)\n",
    "a = min_max_scaler.transform(X_train)\n",
    "X_train_trans = pd.DataFrame(a, columns = cols) # Watch out for putting back in columns here\n",
    "\n",
    "a = min_max_scaler.transform(X_valid)\n",
    "X_valid_trans = pd.DataFrame(a, columns = cols) # Watch out for putting back in columns here\n",
    "\n",
    "a = min_max_scaler.transform(X_test)\n",
    "X_test_trans = pd.DataFrame(a, columns = cols) # Watch out for putting back in columns here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "my_model = sklearn.linear_model.LogisticRegression(max_iter = 1000)\n",
    "my_model.fit(X_train_trans,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make a set of predictions for the test data\n",
    "y_pred = my_model.predict(X_valid_trans)\n",
    "\n",
    "# Print performance details\n",
    "print(sklearn.metrics.classification_report(y_valid, y_pred))\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix\")\n",
    "pd.crosstab(y_valid, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nearest Neighbour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nearest neighbour** models are a lazy  approach to machine learning. The **KNeighborsClassifier** model object in **sklearn** implements logistic regression. The key parmaeters when creating a **KNeighborsClassifier** model are:\n",
    "\n",
    "- **n_neighbors** = 5: Number of neighbors to use.\n",
    "- **weights** =  'uniform': Allows weighted nearest neighbour by setting to 'distance'\n",
    "- **metric** = 'minkowski': The distance metric to copmpare neighbours.\n",
    "- **n_jobs** = 1: Number of jobs to run in parallel. -1 uses all available. \n",
    "- **verbose**=0: Controls how much output will be produced when methods are called - can be 0 (no output), 1, or 2 (maximum output). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Do the same job with random forests\n",
    "my_model = sklearn.neighbors.KNeighborsClassifier()\n",
    "my_model = my_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assess the performance of the decision tree on the **validation set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Make a set of predictions for the test data\n",
    "y_pred = my_model.predict(X_valid)\n",
    "\n",
    "# Print performance details\n",
    "print(sklearn.metrics.classification_report(y_valid, y_pred))\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix\")\n",
    "pd.crosstab(y_valid, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set up the parameter grid to seaerch\n",
    "param_grid = [\n",
    "    {'n_neighbors':[1, 5, 15],\n",
    "     'weights':['uniform', 'distance']}\n",
    "]\n",
    "\n",
    "# Perform the search\n",
    "my_tuned_model = sklearn.model_selection.GridSearchCV(sklearn.neighbors.KNeighborsClassifier(), param_grid, cv=5)\n",
    "my_tuned_model.fit(X_train_plus_valid, y_train_plus_valid)\n",
    "\n",
    "# Print details\n",
    "print(\"Best parameters set found on development set:\")\n",
    "print(my_tuned_model.best_params_)\n",
    "print(my_tuned_model.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient boosting models are especially effective for problems based on tabular data. The **GradientBoostingClassifier** model object in **sklearn** implements the gradient boosting algorithms. The key parmaeters when creating a **GradientBoostingClassifier** model are:\n",
    "\n",
    "- **n_estimators** = 100: The number of boosting stages to perform. Large numbers usually perfrom very well. \n",
    "- **learning_rate** = 0.1: Learning rate shrinks the contribution of trees in later iterations.\n",
    "- **subsample** = 1.0: The fraction of samples to be used for fitting the individual base learners. \n",
    "- **max_features**=None: The number of features to consider when looking for the best split. Can be a number of features, 'sqrt' for square root of total numebr of features, 'log2' for log base 2 of the total number of features, or None for the total number of features. \n",
    "- **min_samples_leaf** = 1: The minimum number of samples required to be at a leaf node in the tress in the ensemble.\n",
    "- **validation_fraction** = 0.1: The proportion of training data to set aside as validation set for\n",
    "    early stopping. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Do the same job with random forests\n",
    "my_model = sklearn.ensemble.GradientBoostingClassifier(n_estimators=300, \\\n",
    "                                           min_samples_split=200)\n",
    "my_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assess the performance of the model on the **validation set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make a set of predictions for the test data\n",
    "y_pred = my_model.predict(X_valid)\n",
    "\n",
    "# Print performance details\n",
    "print(sklearn.metrics.classification_report(y_valid, y_pred))\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix\")\n",
    "pd.crosstab(y_valid, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose parameters using a grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set up the parameter grid to seaerch\n",
    "param_grid = [\n",
    " {'n_estimators': list(range(100, 501, 50)), \n",
    "  'max_features': list(range(1, 10, 2)), \n",
    "  'min_samples_split': list(range(20, 200, 50)) }\n",
    "]\n",
    "\n",
    "# Perform the search\n",
    "my_tuned_model = sklearn.model_selection.GridSearchCV(sklearn.ensemble.GradientBoostingClassifier(), param_grid, cv=5)\n",
    "my_tuned_model.fit(X_train_plus_valid, y_train_plus_valid)\n",
    "\n",
    "# Print details\n",
    "print(\"Best parameters set found on development set:\")\n",
    "print(my_tuned_model.best_params_)\n",
    "print(my_tuned_model.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
