{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Innovate Data Academy\n",
    "[Krisolis](http://www.krisolis.ie)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Predictive Modelling in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates a simple predictive model using **decision trees** in **scikit-learn**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Package Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build predictive models in Python we use a set of libraries that are imported here. In particular **pandas** and **sklearn** are particularly important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# General data handling\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 1000) \n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "import numpy as np\n",
    "\n",
    "# Drawing plots\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Machine learning with scikit-learn\n",
    "import sklearn\n",
    "import sklearn.impute\n",
    "import sklearn.model_selection\n",
    "import sklearn.metrics\n",
    "import sklearn.tree\n",
    "\n",
    "# System packages for saving Python ojects\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To support data exploration and manipulation it is easiest to load datasets as **Pandas DataFrames**. In this example we will load **breast-cancer-wisconsin.csv**. This dataset was collected by analysing cells taken from patients suspected of having breast cancer. The task for the predictive model is to determine whether a sample belong to healthy ('*benign*') cell or a cancerous ('*malignant*') cell. The descriptive features in this dataset are: \n",
    "\n",
    "- **ID**: Unique patient ID\n",
    "- **Clump Thickness**: A measure of clumping present in the cell (measured from 1 - 10)\n",
    "- **Uniformity of Cell Size**: A measure of uniformity of the sizes of cells in the sample (measured from 1 - 10)\n",
    "- **Uniformity of Cell Shape**: A measure of uniformity of the shapes of cells in the sample (measured from 1 - 10)\n",
    "- **Marginal Adhesion**: The degree to which cells stick together (measured from 1 - 10)\n",
    "- **Single Epithelial Cell Size**: The size of a single measured cell  (measured from 1 - 10)\n",
    "- **Bare Nuclei**: The degree to which nuclei devoid of cytoplasm are present int he sample (measured from 1 - 10)\n",
    "- **Bland Chromatin**: The degree of texture present in chromatin (measured from 1 - 10)\n",
    "- **Normal Nucleoli**: Assessment of size of cells  (measured from 1 - 10)\n",
    "- **Mitoses**: The degree to which mitoses are present (measured from 1 - 10)\n",
    "- **Class**: One of 'benign' or 'malignant'. The target feature. \n",
    "\n",
    "Load the data from a csv file to a Pandas DataFrame and examine it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "development_dataset_filename = '../data/breast-cancer-wisconsin.csv'\n",
    "query_dataset_filename = '../data/breast-cancer-wisconsin-query.csv'\n",
    "target_feature_name = 'Class'\n",
    "index_col = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(development_dataset_filename, index_col = index_col)\n",
    "print(dataset.shape)\n",
    "print(dataset.columns.to_list())\n",
    "display(dataset.head())\n",
    "display(dataset.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the distribution of the target feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[target_feature_name].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data for Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two basic jobs that we need to do to prepare datasets for modelling using machine learning:\n",
    "- Separate descriptive features into two sets, one for descriptive features and one for the target feature.\n",
    "- Divide the data into two samples, one for training the model and one for evaluating it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separate Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**sklearn** expects datasets to be contained in two sets: one for descriptive features (typically called `X`) and one for the target feature (typically called `y`). Here we extract the descriptive features for `X` and the target feature for `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = dataset.loc[:, dataset.columns != target_feature_name]\n",
    "y = dataset[target_feature_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the `X` and `y` DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(X.columns.to_list())\n",
    "display(X.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y.shape)\n",
    "display(y.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Partition Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When building machine learning models it is very common to divide a dataset into **training**, **validation** and **test** partitions. These partitions are used for different roles in modelling. The **train_test_split** function from **sklearn** provides easy functionality to do this. The main parameters of the **train_test_split** function are:\n",
    "\n",
    "- **X**: The dataset descriptive feature values.\n",
    "- **y**: The dataset target feature values.\n",
    "- **test_size**=None: The size of the test partition (either between 0.0 and 1.0 for a percentage o the data, or an actual number of instances). \n",
    "- **train_size**=None: The size of the training partition (either between 0.0 and 1.0 for a percentage o the data, or an actual number of instances). \n",
    "- **shuffle**=True: Should the data be shuffled before being partitioned?\n",
    "- **stratify**=None: Allows us to set a target feature so that its distributions remain the same in the created partitions. \n",
    "\n",
    "We create a **training set** and a **validation set**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid \\\n",
    "    = sklearn.model_selection.train_test_split(X, \n",
    "                                               y, \n",
    "                                               random_state = 0, \n",
    "                                               train_size = 0.7,\n",
    "                                               stratify = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the partitions created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_train.columns.to_list())\n",
    "display(X_train.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_valid.shape)\n",
    "print(X_valid.columns.to_list())\n",
    "display(X_valid.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train.shape)\n",
    "display(y_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_valid.shape)\n",
    "display(y_valid.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling: A Very Simple Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train a classification model **sklearn** uses a two-step process:\n",
    "\n",
    "1. Create the model object with hyperparameters\n",
    "2. Fit the model to the data (using the **fit** function). \n",
    "\n",
    "In this first example we will build a decision tree using the **DecisionTreeClassifier** object from **sklearn**. The key parameters passed when creating a DecisionTreeClassifier are:\n",
    "\n",
    "- **criterion** = \"gini\": the criterion used for selecting partitions during training. One of either \"entropy\" or \"gini\".\n",
    "- **splitter** = \"best\": The approach used to split numeric data at each node in the tree. One of either \"random\" or \"best\".\n",
    "- **max_depth** = None: The maximum depth that the tree is allowed to grow to. \n",
    "- **min_samples_split** = 2: The minimum number of samples required to split an internal node. \n",
    "- **min_samples_leaf** = 1: The minimum number of samples required to be at a leaf node.\n",
    "- **class_weight** = None: A set of weights for classes - often used to handle imbalanced target values. \n",
    "- **ccp_alpha** = 0.0: Complexity parameter used for Minimal Cost-Complexity Pruning. \n",
    "\n",
    "We create a simple decision tree using the '*entropy*' splitting criterion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "my_tree = sklearn.tree.DecisionTreeClassifier(criterion=\"entropy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the model using the **fit** method passing this the training data sample created earlier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the decision tree in a nice text format using the sklearn **export_text** function. The key parameters passed to **export_text** are:\n",
    "\n",
    "- **decisiontree**: The tree to be plotted.\n",
    "- **feature_names** = None: A list of the names of the features used in the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sklearn.tree.export_text(my_tree, \n",
    "                               feature_names=X_train.columns.to_list()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the advantages of using decision trees is that we can draw nice pictures of them. The **plot_tree** method from the **DecisionTreeCalssfiier** object in **sklearn** does a nice job of this. The key parameters passed to **plot_tree** are:\n",
    "\n",
    "- **decisiontree**: The tree to be plotted.\n",
    "- **feature_names** = None: A list of the names of the features used in the tree.\n",
    "- **class_names** = None: Names of each of the target classes in ascending numerical order.\n",
    "- **filled** = False: Whether or not to colour the tree based on the target distribution at each node. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10))\n",
    "_ = sklearn.tree.plot_tree(my_tree, \n",
    "                           feature_names = X_train.columns,\n",
    "                           class_names = my_tree.classes_,\n",
    "                           filled = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To assess the performance of a trained model we typically use it to make predictions on a dataset for which we know the correct target values. For any **sklearn** model we use the **predict** method to make predictions. The key parameter for **predict** is **X**, which is the data for which predictions will be made. Predict returns a list of predictions - one for each row in **X**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_pred = my_tree.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us back a list of predictions, one for each row in the **X** DataFrame. (Note these predictions are returned as a Numpy array, not a Pandas DataFrame.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have a set of predictions we can calculate performance measures by comparing these to the ground truth. **sklearn** provides functions for a wide range of performance measures.  They almost all follow the same pattern - we provide them a list of ground truth target values and a list of predictions. Here we calculate simple accuracy using **accuracy_score**. The key parameters passed to **accuracy_score** are:\n",
    "\n",
    "- **y_true**: The ground truth for predictions.\n",
    "- **y_pred**: The predictions made by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = sklearn.metrics.accuracy_score(y_train, y_pred) \n",
    "print(\"Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks very good, but maybe too good to be true! Rather than assessing the performance of the tree on the training data sample we should do it on another sample that was not used during model training. Here we assess the performance of the tree on the **validation dataset**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_pred = my_tree.predict(X_valid)\n",
    "accuracy = sklearn.metrics.accuracy_score(y_valid, y_pred) \n",
    "print(\"Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a reasonably large difference between the accuracy score achieved with the training data sample and the accuracy achieved using the validation data sample. This is evidence of **overfitting**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Alternative Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can change the hyperparmaeters used by a machine learning model to, sometimes quite dramatically, change its behaviour. Here we create another decision tree this time limiting its depth to 2 using the **max_depth** parameter. **Max_depth** is an example of a hyperparameter used in the decision tree algorithm, it only allows the algorithm om build trees with 2 levels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "my_tree = sklearn.tree.DecisionTreeClassifier(criterion = \"entropy\", \n",
    "                                              max_depth = 2)\n",
    "my_tree = my_tree.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the decision tree. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sklearn.tree.export_text(my_tree, \n",
    "                               feature_names=list(X_train.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualise the decision tree so we can see what it is doing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10))\n",
    "_ = sklearn.tree.plot_tree(my_tree, \n",
    "                           feature_names = X_train.columns,\n",
    "                           class_names = my_tree.classes_,\n",
    "                           filled = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assess the performance of the decision tree on the **training set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_pred = my_tree.predict(X_train)\n",
    "\n",
    "accuracy = sklearn.metrics.accuracy_score(y_train, y_pred) \n",
    "print(\"Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assess the performance of the decision tree on the **validation set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_pred = my_tree.predict(X_valid)\n",
    "\n",
    "accuracy = sklearn.metrics.accuracy_score(y_valid, y_pred) \n",
    "print(\"Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A useful rule of thumb is to only allow trees to grow as far as having 5% - 10% of data at any leaf node. We can set this using the **min_samples_split** hyperparameter which can be given a percentage of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "my_tree = sklearn.tree.DecisionTreeClassifier(criterion = \"entropy\", \n",
    "                                              min_samples_split = 0.1)\n",
    "my_tree = my_tree.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualise the decision tree so we can see what it is doing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10))\n",
    "_ = sklearn.tree.plot_tree(my_tree, \n",
    "                           feature_names = X_train.columns,\n",
    "#                           class_names = my_tree.classes_,\n",
    "                           filled = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assess the performance of the decision tree on the **training set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_pred = my_tree.predict(X_train)\n",
    "\n",
    "accuracy = sklearn.metrics.accuracy_score(y_train, y_pred) \n",
    "print(\"Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assess the performance of the decision tree on the **validation set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_pred = my_tree.predict(X_valid)\n",
    "\n",
    "accuracy = sklearn.metrics.accuracy_score(y_valid, y_pred) \n",
    "print(\"Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest way to deploy a machine learning model is to save the trained model out to a file using the Python **pickle** package. From the **pickle** package the **dump** method writes the data of any Python object to a file. The key parameters of the method are:\n",
    "\n",
    "- **obj**: The object to write to the file.\n",
    "- **file**: A file object to write to\n",
    "\n",
    "We open the file first using the **open** method to which we pass a file name and the '*wb*' mode to write with binary data. It's important to remember to close the file using the **close** method after writing the object to the file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fileObject = open(\"my_model.bin\",'wb') \n",
    "pickle.dump(my_tree, fileObject)   \n",
    "fileObject.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can reload the saved object using the **load** method from the **pickle** package. It takes a file object as its only parameter and returns the loaded object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fileObject = open('my_model.bin','rb')  \n",
    "loaded_model = pickle.load(fileObject)  \n",
    "fileObject.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the model we can now load a dataset containing only the descriptive features so that we can make predictions for it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_query = pd.read_csv(query_dataset_filename, index_col = 0)\n",
    "print(X_query.shape)\n",
    "print(X_query.columns.to_list())\n",
    "display(X_query.head())\n",
    "display(X_query.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a simple way to use the model outputs we can make predictions for the data loaded and write the results out to a file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = loaded_model.predict(X_query)\n",
    "predictions = pd.DataFrame({'customer' : X_query.index, 'prediction' : y_pred})\n",
    "print(predictions.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "47.7031px",
    "left": "1202px",
    "top": "67.1406px",
    "width": "159.359px"
   },
   "toc_section_display": false,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
