{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECB Data Academy - Week 3 - Supervised Learning\n",
    "[Krisolis](http://www.krisolis.ie)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â Evaluating Predictive Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how predictive models are evaluated in sklearn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Package Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build predictive models in Python we use a set of libraries that are imported here. In particular **pandas** and **sklearn** are particularly important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Saving Python ojects\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# General data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Drawing plots\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#import pandas_profiling\n",
    "\n",
    "# Machine learning\n",
    "import sklearn\n",
    "import sklearn.impute\n",
    "import sklearn.model_selection\n",
    "import sklearn.metrics\n",
    "import sklearn.tree\n",
    "import sklearn.svm\n",
    "import sklearn.ensemble\n",
    "import sklearn.linear_model\n",
    "import sklearn.neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load & Partition Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To support data exploration and manipulation it is easiest o load datasets as **pandas DataFrames**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('../data/ACME_ABT.csv')\n",
    "display(dataset.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the distribution of the two classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset[\"churn\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a suite of summary statistics for the numeric and categorical features in the data, and count missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Print descriptive statsitcs for each column\n",
    "print(\"Summary Stats\")\n",
    "if dataset.select_dtypes(include=[np.number]).shape[1] > 0: \n",
    "    display(dataset.describe(include=\"number\").transpose())\n",
    "if dataset.select_dtypes(include=[object]).shape[1] > 0: \n",
    "    display(dataset.describe(include=\"object\").transpose())\n",
    "\n",
    "# Check for presence of missing values\n",
    "print(\"Missing Values\")\n",
    "print(dataset.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **ProfileReport** from **pandas_profiling** gives a very useful summary of the dataset and highliughts potential issues. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#pandas_profiling.ProfileReport(dataset, \n",
    "                               #minimal = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A bug in pandas_profiler means plots don;t appear after calling it. This re-import of matplotlib fixes the bug.\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare Data for Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We select features, imp-iuts missing values, replace spurious values and change categorical features to numeric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = dataset[['age',\n",
    " 'income',\n",
    " 'numHandsets',  \n",
    " 'handsetAge',\n",
    " 'smartPhone',\n",
    " 'currentHandsetPrice',\n",
    " 'avgBill',\n",
    " 'avgOverBundleMins',\n",
    " 'avgRoamCalls',\n",
    " 'callMinutesChangePct',\n",
    " 'callMinutesChangePct',\n",
    " 'billAmountChangePct',\n",
    " 'billAmountChangePct',\n",
    " 'avgReceivedMins',\n",
    " 'avgOutCalls',\n",
    " 'avgInCalls',\n",
    " 'peakOffPeakRatio',\n",
    " 'peakOffPeakRatioChangePct',\n",
    " 'avgDroppedCalls',\n",
    " 'avgDroppedCalls',\n",
    " 'lifeTime',\n",
    " 'newFrequentNumbers',\n",
    " 'regionType',\n",
    " 'marriageStatus',\n",
    " 'creditRating']]\n",
    "y = dataset[\"churn\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.loc[X['regionType'] == 's','regionType'] = \"suburban\"\n",
    "X.loc[X['regionType'] == 't','regionType'] = \"town\"\n",
    "X.loc[X['regionType'] == 'r','regionType'] = \"rural\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regionType_imputer = sklearn.impute.SimpleImputer(strategy=\"most_frequent\")\n",
    "regionType_imputer.fit(X['regionType'].values.reshape(-1, 1))\n",
    "X['regionType'] = regionType_imputer.transform(X['regionType'].values.reshape(-1, 1))\n",
    "\n",
    "age_imputer = sklearn.impute.SimpleImputer(missing_values = 0, strategy=\"mean\")\n",
    "age_imputer.fit(X['age'].values.reshape(-1, 1))\n",
    "X['age'] = age_imputer.transform(dataset['age'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creditRating_oe = sklearn.preprocessing.OrdinalEncoder()\n",
    "creditRating_oe.fit(X['creditRating'].values.reshape(-1, 1))\n",
    "X['creditRating'] = creditRating_oe.transform(X['creditRating'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.get_dummies(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examine Transformed Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the transformed dataset before modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "display(X.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pandas_profiling.ProfileReport(X, \n",
    "                               minimal = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A bug in pandas_profiler means plots don;t appear after calling it. This re-import of matplotlib fixes the bug.\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Partition Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into a **training set**, a **validation set**, and a **test set**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train_plus_valid, X_test, y_train_plus_valid, y_test \\\n",
    "    = sklearn.model_selection.train_test_split(X, y, random_state=0,\n",
    "                                               train_size = 0.7, \n",
    "                                               stratify = y)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid \\\n",
    "    = sklearn.model_selection.train_test_split(X_train_plus_valid, \n",
    "                                               y_train_plus_valid, \n",
    "                                               random_state=0, \n",
    "                                               train_size = 0.5/0.7,\n",
    "                                               stratify=y_train_plus_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the partitions created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "display(X_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_valid.shape)\n",
    "display(X_valid.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(y_train.shape)\n",
    "display(y_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train a simple decision tree model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "my_tree = sklearn.tree.DecisionTreeClassifier(criterion=\"entropy\", \n",
    "                                              max_depth=5)\n",
    "my_tree = my_tree.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualise the decision tree so we can see what it is doing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10))\n",
    "_ = sklearn.tree.plot_tree(my_tree, \n",
    "                feature_names=X_train.columns,\n",
    "                  # class_names=iris.target_names,\n",
    "                   filled=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our standard evalaution block for classification problems generates the classification report and a confusion matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a set of predictions for the training data\n",
    "y_pred = my_tree.predict(X_train)\n",
    "\n",
    "# Print performance details\n",
    "print(sklearn.metrics.classification_report(y_train, y_pred))\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix\")\n",
    "pd.crosstab(y_train, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a set of predictions for the training data\n",
    "y_pred = my_tree.predict(X_valid)\n",
    "\n",
    "# Print performance details\n",
    "print(sklearn.metrics.classification_report(y_valid, y_pred))\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix\")\n",
    "pd.crosstab(y_valid, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using **sklearn** we can calculate a whole raft of performance measures using the same pattern that we used for accuracy. For example here we use the **f1_score** and **brier_score** functions from **sklearn** to cauiclate different perfomrance measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score = sklearn.metrics.f1_score(y_valid, y_pred) \n",
    "print(\"F1 Score: \" +  str(f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brier_score = sklearn.metrics.brier_score_loss(y_valid, y_pred) \n",
    "print(\"Brier Score: \" +  str(brier_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ROC curves** are a useful tool for evalauting the performance of machine learning models trained for binary classification problems (although with the simple decision tree in this exmaple they are not terribly useful). To generate an ROC curve we first have to generate **prediction score** output fromt he trasined model rahter than actual class labels. In **sklearn** we do this using the **predict_proba** method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_score = my_tree.predict_proba(X_valid)\n",
    "display(y_pred_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **roc_auc_score** method from **sklearn** will genrate the infrmation needed to generate an ROC cource. It is called in the same way as accuracy - by passing a list of ground truth target labels and a set of predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(sklearn.metrics.roc_auc_score(y_valid, y_pred_score[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that **y_pred_score** contains two columns (one for each class) but we only need one of these for generating prediction score-based performance measures. To extract this we use `y_pred_score[:, 1]`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To extract the information required to draw an ROC curve **sklearn** provides the **roc_curve** merthod. The returns the false positive rate and true positive rates for a set of thresholds (which are also returned). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresh = sklearn.metrics.roc_curve(y_valid, y_pred_score[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the false positive rate and true positive rate we can caluclate the ROC index as well as drawing the actual **ROC curve**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc = sklearn.metrics.auc(fpr, tpr)\n",
    "print(roc_auc)\n",
    "plt.plot(fpr, tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Using Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative to using a single trianing and validation set partition to evaluate a machine learning model is to use  ***k*-fold cross valdiation**. We do this in **sklearn** using the **cross_val_score** method. The key parameters for the **cross_val_score** method are:\n",
    "\n",
    "- **estimator**: The model object to fit to the data.\n",
    "- **X**: The descriptive feature  data to fit to.\n",
    "- **y**: the target feature values to fit to.\n",
    "- **cv** = 5: The number of folds to perform.\n",
    "- **scoring** = None: The scoring method to use to assess models.\n",
    "- **n_jobs** = 1: Number of jobs to run in parallel. -1 uses all available. \n",
    "- **verbose**=0: Controls how much output will be produced when methods are called - can be 0 (no output), 1, or 2 (maximum output). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = sklearn.model_selection.cross_val_score(\n",
    "                                    sklearn.tree.DecisionTreeClassifier(\n",
    "                                             criterion = 'entropy', max_depth = 5), \n",
    "                                    X_train_plus_valid, y_train_plus_valid, \n",
    "                                    cv=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function returns the cross valdiation scores, which we typically aggregate using an average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scores)\n",
    "print('Mean: {} Std. dev.: {}'.format(np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can change the perfomrance measure used using the **scoring** parameter. there is a list of those that can be used here: https://scikit-learn.org/stable/modules/model_evaluation.html In this exmaple we swap to using AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = sklearn.model_selection.cross_val_score(\n",
    "                                    sklearn.tree.DecisionTreeClassifier(\n",
    "                                             criterion = 'entropy', max_depth = 5), \n",
    "                                    X_train_plus_valid, y_train_plus_valid, \n",
    "                                    scoring = 'roc_auc',\n",
    "                                    cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scores)\n",
    "print('Mean: {} Std. dev.: {}'.format(np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing Hyper-parameters Using a Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A grid search is a useful way to set hyper-parameter values for machine learning models. The **sklearn** **GridSearchCV** object implements this. To perform a grid search we first set up a parameter search grid as a dictionary where each key is a parameter name and each value is a list of options to be searched. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'criterion': ['gini', \"entropy\"], \n",
    "              'splitter': ['best', 'random'],\n",
    "              'max_depth': list(range(3, 20, 3))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform the actual grid search we follow the usual **sklearn** pattern of creating an object and then fitting it to a dataset using the **fit** function. The key parmaeters when creating a **GridSearchCV** object are:\n",
    "\n",
    "- **estimator**: The model object to fit to the data.\n",
    "- **param_grid**: The parameter grid to search. \n",
    "- **scoring** = None: The scoring method to use to assess models.\n",
    "- **n_jobs** = 1: Number of jobs to run in parallel. -1 uses all available. \n",
    "- **cv** = 5: The number of folds to perform.\n",
    "- **refit** = True: Refit an estimator using the best found parameters on the whole dataset.\n",
    "- **verbose**=0: Controls how much output will be produced when methods are called - can be 0 (no output), 1, or 2 (maximum output). \n",
    "\n",
    "To the **fit** function we pass the dataset to use (in this case the combined training and validation partitions). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "my_tuned_tree = \\\n",
    "    sklearn.model_selection.GridSearchCV( \\\n",
    "            sklearn.tree.DecisionTreeClassifier(min_samples_split=50),\n",
    "            param_grid, \n",
    "            cv=10, \n",
    "            scoring = 'roc_auc', \n",
    "            verbose = 2, \n",
    "            n_jobs=1)\n",
    "my_tuned_tree.fit(X_train_plus_valid, y_train_plus_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The grid search returns a data structure containing the following attributes:\n",
    "    \n",
    "- **cv_results_**: A dictionary of the performance results for the grid search. \n",
    "- **best_estimator_**: The model chosen by the saerch. \n",
    "- **best_score_**: Mean cross-validated score of the best_estimator.\n",
    "- **best_params_**: Parameter setting that gave the best results on the hold out data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best score: {}\".format(my_tuned_tree.best_score_))\n",
    "print(\"Best parameters: {}\".format(my_tuned_tree.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display(my_tuned_tree.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualise the final tree found by accessing the **best_estimator_** attribute which accesses the trained tree object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "viz = dtreeviz.trees.dtreeviz(my_tuned_tree.best_estimator_, \n",
    "                              X_train_plus_valid, y_train_plus_valid,\n",
    "                              target_name=\"churn\",\n",
    "                              feature_names=X_train.columns)\n",
    "viz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the final model determined we can now make a set of predictions for the hold-out test set to evaluate the generalisation error of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Make a set of predictions for the test data\n",
    "y_pred = my_tuned_tree.predict(X_test)\n",
    "\n",
    "# Print performance details\n",
    "print(sklearn.metrics.classification_report(y_test, y_pred))\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix\")\n",
    "pd.crosstab(y_test, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
